{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YIbckW5YZnkb"
      },
      "outputs": [],
      "source": [
        "! pip -q install mistralai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')"
      ],
      "metadata": {
        "id": "uPH_UgNec-Av"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(\"Mistral API Key setup complete.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "tZj5SD5TeNN1",
        "outputId": "f46f1fb4-a526-432c-8582-6baa63012bd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Mistral API Key setup complete."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai import Mistral\n",
        "\n",
        "\n",
        "model = \"mistral-small-2501\" #mistral-small-2501\n",
        "# model = \"mistral-small-latest\"\n",
        "\n",
        "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=[{\"role\":\"user\", \"content\":\"What is the best French cheese?\"}]\n",
        ")\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVcPqqgNehzY",
        "outputId": "68326cd1-ee7c-4698-919f-e70041219f3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choosing the \"best\" French cheese often depends on personal preference, as France has a wide variety of cheeses with distinct flavors, textures, and aromas. However, some of the most renowned and beloved French cheeses include:\n",
            "\n",
            "1. **Camembert**: A soft, creamy cheese with a bloomy rind, often associated with Normandy. It has a rich, buttery taste that can vary from mild to quite strong.\n",
            "\n",
            "2. **Brie**: Another soft cheese with a bloomy rind, Brie is known for its creamy interior and mild, nutty flavor. It is often served at room temperature to allow its flavors to fully develop.\n",
            "\n",
            "3. **Roquefort**: A blue cheese made from sheep's milk, Roquefort is known for its pungent, tangy flavor and crumbly texture. It is often served with crackers or bread.\n",
            "\n",
            "4. **Comté**: A hard, uncooked cheese made from cow’s milk. It has a rich, nutty flavor and a firm texture, making it a versatile option for cooking or snacking.\n",
            "\n",
            "5. **Chèvre**: A type of goat cheese that comes in various forms, from soft and creamy to firm and crumbly. Its flavor can range from mild to quite strong, depending on the variety.\n",
            "\n",
            "6. **Pont-l'Évêque**: A soft, washed-rind cheese from Normandy, known for its rich, creamy texture and slightly pungent aroma.\n",
            "\n",
            "7. **Mimolette**: A hard, aged cheese with a unique, slightly crunchy texture and a nutty, slightly sweet flavor. It is often used as a grated cheese in cooking.\n",
            "\n",
            "8. **Époisses**: A soft, washed-rind cheese with a strong, pungent aroma and a creamy, slightly salty flavor. It is often served as a table cheese.\n",
            "\n",
            "Each of these cheeses has its own unique characteristics, and the \"best\" one really depends on your personal taste preferences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using it with **LangChain**"
      ],
      "metadata": {
        "id": "6h0G7PlgfFjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U langchain-core langchain-mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZoWHdpFeubS",
        "outputId": "47020298-7bf4-459d-9a5f-be84f05d6c6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/412.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "DA_dzOz-fJ9r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If mistral_api_key is not passed, default behavior is to use the `MISTRAL_API_KEY` environment variable.\n",
        "chat = ChatMistralAI(\n",
        "    model=\"mistral-small-2501\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=128,\n",
        "                     )"
      ],
      "metadata": {
        "id": "-lNj9VwIfN82"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content='Where does \"hello world\" come from?')]\n",
        "response = chat.invoke(messages)\n",
        "\n",
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "si0-ucF9fSgN",
        "outputId": "2df75069-9b15-402c-be92-08dd2a795e40"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The phrase \"Hello, World!\" is a classic introductory expression used in programming and computer science. It originated from the famous book \"The C Programming Language\" by Brian Kernighan and Dennis Ritchie, often referred to as \"K&R,\" which was published in 1978. In this book, the authors used \"hello, world\" as a simple example to demonstrate how to write a basic program in the C programming language.\n\nThe program typically looks like this in C:\n\n```c\n#include <stdio.h>\n\nint main() {\n    printf(\"hello, world\\n\");\n    return 0;\n}\n```\n\nThis simple program prints"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model"
      ],
      "metadata": {
        "id": "afg35h-zfboh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "mUs3ECSVfVcd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(input_text, system_prompt=\"\",max_length=512):\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=input_text)\n",
        "        ]\n",
        "    chat = ChatMistralAI(\n",
        "        model=\"mistral-small-2501\",\n",
        "        temperature=0.7,\n",
        "        max_tokens=max_length,\n",
        "                     )\n",
        "    response = chat.invoke(messages)\n",
        "    text = response.content\n",
        "    wrapped_text = wrap_text(text)\n",
        "    display(Markdown(wrapped_text))"
      ],
      "metadata": {
        "id": "bY9eGCzbfgCT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "TLQkVQWefi4E",
        "outputId": "9950a1f3-c6ae-4213-b4e0-0efacdb22022"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To create a detailed analogy between mathematics and a lighthouse, we need to break down\nthe key components and functions of both and find corresponding elements. Here's a step-\nby-step reasoning:\n\n1. **Purpose and Guidance:**\n   - **Lighthouse:** A lighthouse is built to guide ships safely to shore and warn them of\ndangerous areas.\n   - **Mathematics:** Similarly, mathematics provides a framework for understanding and\nnavigating the complexities of the world. It guides us in making sense of patterns,\nstructures, and relationships.\n\n2. **Light (Illumination):**\n   - **Lighthouse:** The beam of light from a lighthouse illuminates the path for ships,\nmaking it easier for them to navigate through darkness and fog.\n   - **Mathematics:** Mathematical principles and concepts \"illuminate\" problems, making\nthem easier to understand and solve. They cut through the \"fog\" of complexity and\nambiguity.\n\n3. **Foundation and Stability:**\n   - **Lighthouse:** A lighthouse is firmly rooted and built to withstand harsh\nconditions, providing a reliable reference point.\n   - **Mathematics:** Mathematical principles are foundational and reliable, providing a\nstable base for further exploration and discovery in various fields such as physics,\nengineering, and computer science.\n\n4. **Navigation (Problem-Solving):**\n   - **Lighthouse:** Sailors use the light from a lighthouse to navigate and stay on\ncourse.\n   - **Mathematics:** Problem-solvers use mathematical tools and techniques to navigate\nthrough complex problems and find solutions.\n\n5. **Patterns and Repetition:**\n   - **Lighthouse:** The rotating light of a lighthouse creates a recognizable pattern\nthat sailors can use to identify their location.\n   - **Mathematics:** Mathematical concepts often involve patterns and repetition, such as\nin sequences, fractals, and cyclic functions, which help in recognizing and predicting\nphenomena.\n\n6. **Warning System:**\n   - **Lighthouse:** A lighthouse warns ships of dangerous areas and helps them avoid\nhazards.\n   - **Mathematics:** Mathematical models and simulations can warn of potential risks and\nhazards in real-world scenarios, helping to prevent disasters and optimize outcomes.\n\n7. **Structure and Design:**\n   - **Lighthouse:** The structure of a lighthouse is carefully designed to be tall,\nvisible, and resilient.\n   - **Mathematics:** Mathematical theories and structures are meticulously constructed to\nbe logical, consistent, and applicable in various contexts.\n\n8. **Universality:**\n   - **Lighthouse:** A lighthouse serves as a universal guide for all ships in its\nvicinity, regardless of their origin or destination.\n   - **Mathematics:** Mathematical principles are universal, applicable to all domains of\nknowledge and applicable across different cultures and times.\n\nBy breaking down these components, we can see that both mathematics and a lighthouse serve\nas guiding beacons, providing clarity, stability, and direction in their respective\ndomains."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Write a detailed analogy between mathematics and a music.',\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "Gn0rLl_-foHh",
        "outputId": "2432d0d7-1b5e-4414-c4b1-5e104d1cfd31"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To create a detailed analogy between mathematics and music, we'll compare various elements\nof both fields. Here's a step-by-step breakdown:\n\n1. **Structure and Rules:**\n   - **Mathematics:** Has a set of rules and axioms that govern how operations are\nperformed. For example, the commutative, associative, and distributive properties of\narithmetic.\n   - **Music:** Has rules and structures too, such as time signatures, key signatures, and\nchord progressions. These rules guide how notes and rhythms are combined to create harmony\nand melody.\n\n2. **Language and Notation:**\n   - **Mathematics:** Uses a specific language and symbols to represent concepts and\noperations, such as algebra, calculus, and geometry.\n   - **Music:** Uses sheet music, which is a visual language that represents pitch,\nrhythm, and dynamics.\n\n3. **Infinite Possibilities:**\n   - **Mathematics:** Has an infinite number of problems to solve, theorems to prove, and\nconcepts to explore.\n   - **Music:** Offers an infinite number of melodies, harmonies, and rhythms to create,\nblending notes and sounds in various ways.\n\n4. **Themes and Patterns:**\n   - **Mathematics:** Involves identifying and working with patterns, such as sequences,\nseries, and fractals.\n   - **Music:** Is about creating and recognizing patterns, such as rhythms, melodies, and\nchord progressions.\n\n5. **Exploration and Discovery:**\n   - **Mathematics:** Often involves exploration and discovery, such as investigating a\nproblem or concept to see where it leads.\n   - **Music:** Often involves experimentation and exploration, trying out different\nnotes, chords, or rhythms to see what sounds good.\n\n6. **Theoretical and Experimental Foundation:**\n   - **Mathematics:** Has a theoretical foundation with concepts like calculus, algebra,\nand geometry, but also has experimental aspects through computational mathematics and data\nanalysis.\n   - **Music:** Has a theoretical basis through music theory, but also has an experimental\nside through improvisation and creating new sounds.\n\n7. **Composers and Mathematicians:**\n   - **Mathematics:** Mathematicians create or discover new concepts, theorems, and\nproofs.\n   - **Music:** Composers create new melodies, harmonies, and rhythms, blending notes to\ncreate meaningful music.\n\n8. **Proofs and Performances:**\n   - **Mathematics:** A proof is a step-by-step argument that shows a statement is true\nbased on established rules and axioms.\n   - **Music:** A performance is a presentation of a composition, following the rules of\nrhythm, pitch, and dynamics.\n\n9. **Beauty and Aesthetics:**\n   - **Mathematics:** Mathematical structures, proofs, and concepts can be considered\nbeautiful due to their elegance, simplicity, and complexity.\n   - **Music:** Music is created to be beautiful and pleasing to the ear, invoking\nemotions and evoking sentiments.\n\n10. **Universality:**\n   - **Mathematics:** Mathematical concepts can be universal and applicable across various\ncultures and languages.\n   - **Music:** Music can transcend cultural and linguistic boundaries, evoking emotions\nlike understanding and empathy.\n\nThrough these analogies, we can appreciate the similarities and connections between\nmathematics and music, offering new perspectives on both."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 235 ms, sys: 9.18 ms, total: 244 ms\n",
            "Wall time: 5.72 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
        "         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "iz378mBsfxJX",
        "outputId": "0b2ec64d-9b20-406e-c7a9-fc1e3d104f05"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To understand the difference between a Llama, Vicuna, and an Alpaca, let's break down\ntheir characteristics step-by-step:\n\n1. **Taxonomic Classification**:\n   - All three animals belong to the family Camelidae and the genus Lama.\n   - This means they are all closely related and share many similarities.\n\n2. **Llama (Lama glama)**:\n   - **Size**: Llamas are typically the largest of the three, with adults standing about 4\nfeet (1.2 meters) tall at the shoulder and weighing between 280 to 450 lbs (127 to 204\nkg).\n   - **Purpose**: They are often used as pack animals in South America, carrying goods\nover long distances.\n   - **Appearance**: Llamas have a variety of coat colors, including brown, black, white,\nand various shades in between. They have long faces and large, erect ears.\n   - **Temperament**: Known for being gentle and calm, but can be stubborn at times.\n\n3. **Alpaca (Vicugna pacos)**:\n   - **Size**: Alpacas are smaller than llamas, with adults standing about 3 feet (0.9\nmeters) tall at the shoulder and weighing between 100 to 180 lbs (45 to 82 kg).\n   - **Purpose**: Alpacas are primarily raised for their fiber, which is soft, warm, and\nluxurious.\n   - **Appearance**: Alpacas have short, pointed ears and a more compact, rounded body\ncompared to llamas. They come in a variety of colors and patterns, including white, black,\nbrown, and grey.\n   - **Temperament**: Generally gentle and easy to handle.\n\n4. **Vicuna (Vicugna vicugna)**:\n   - **Size**: Vicuñas are the smallest of the three, with adults standing about 20-30\ninches (50-75 cm) tall at the shoulder and weighing between 77 to 121 lbs (35 to 55 kg).\n   - **Purpose**: Vicuñas are prized for their extremely fine and valuable fleece, known\nas vicuña fiber.\n   - **Appearance**: Vicuñas have a slender build, a light brown or tan coat, and a white\nunderside. They have a shorter, more delicate face and large, pointed ears.\n   - **Temperament**: Very shy and wild; they are not typically domesticated.\n   - **Habitat**: Vicuñas are native to the Andes Mountains in South America and are\nconsidered a wild species.\n\n**Summary**:\n- Llamas are the largest and primarily used as pack animals.\n- Alpacas are smaller and raised for their fiber.\n- Vicuñas are the smallest, have the finest fiber, and are wild animals native to the\nAndes.\n\nBy following these steps, you can clearly distinguish between a Llama, Vicuna, and an\nAlpaca."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 232 ms, sys: 3.24 ms, total: 235 ms\n",
            "Wall time: 4.81 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "Qyteu2lzf0u4",
        "outputId": "ffe8ae25-bbf6-4b6a-936b-49defb9996a1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Subject: Proposal to Open Source GPT-4\n\nDear Sam,\n\nI hope this email finds you well. I am writing to discuss the potential benefits of open\nsourcing GPT-4. While OpenAI has historically used a closed-source model, there are\nseveral strategic advantages to considering an open-source approach for GPT-4:\n\n1. **Community Engagement and Innovation**: Open sourcing GPT-4 would foster a global\ncommunity of developers, researchers, and enthusiasts who can contribute to its\nimprovement. This collective intelligence could accelerate innovation and lead to\nunexpected advancements.\n\n2. **Transparency and Trust**: By open sourcing the model, OpenAI can build greater trust\nwith users and the broader public. Transparency can help alleviate concerns about bias,\nmisuse, and ethical implications, demonstrating OpenAI's commitment to responsible AI\ndevelopment.\n\n3. **Education and Learning**: An open-source GPT-4 would serve as an invaluable\neducational tool. Researchers and students can study the model, understand its inner\nworkings, and develop new methodologies, fostering a deeper understanding of AI within the\nacademic community.\n\n4. **Ecosystem Development**: Open sourcing could stimulate the development of a rich\necosystem of tools, plugins, and applications built around GPT-4. This could create new\nopportunities for collaboration and commercialization, benefiting both the AI community\nand OpenAI.\n\n5. **Competitive Advantage**: While it might seem counterintuitive, open sourcing GPT-4\ncould provide a competitive edge. It allows OpenAI to focus on cutting-edge research while\nleveraging community contributions to enhance the model's capabilities and usability.\n\n6. **Ethical Considerations**: An open-source approach can help mitigate ethical risks by\nallowing the community to identify and address potential issues proactively. This\ncollaborative effort can ensure that GPT-4 is developed and deployed responsibly.\n\nI understand that open sourcing involves significant considerations, including\nintellectual property, security, and the potential for misuse. However, I believe that\nwith careful planning and community involvement, these challenges can be managed\neffectively.\n\nThank you for considering this proposal. I would be happy to discuss it further and\nexplore the feasibility and potential pathways for open sourcing GPT-4.\n\nBest regards,\n\n[Your Name]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 217 ms, sys: 14 ms, total: 231 ms\n",
            "Wall time: 4.42 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Freddy a young 5 year old boy who is scared AI will end the world, write only with the language of a young child!\",\n",
        "         max_length=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "_m0qsXuKf3Ym",
        "outputId": "ce704559-cf3c-49ec-a543-3bfe0ebfe488"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi Mr. Sam,\n\nYou know, I'm just a little kid, and I don't know lots of big words like you. But I heard\nyou got something called GPT-4, and it's like a really smart robot that can talk to us. I\nheard that some people are scared it might do bad stuff, and I want to tell you my\nthoughts.\n\nI think you should let everyone see how GPT-4 works, like a big game of show and tell.\nBecause if all the good people get to see how it works, then they know how to fix it up if\nit goes wrong.  They can see if it's going to end the world or not, because I heard that\nif I know how something works, I can fix it if it breaks.\n\nAlso, if everyone knows how it works, then they can make it even better, like a big team\nof friends working on a big project!  They can get rid of the bad stuff, and keep the good\nstuff!\n\nAnd this way, if it does something bad, we know who did it.  So we can fix it and make it\nbetter.  That way, no one gets in big trouble.\n\nSo, that's what I think. I hope you think it's a good idea too! I just don't want it to\nend the world.\n\nThank you for listening!\n\nYour friend,\nFreddy"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 220 ms, sys: 6.77 ms, total: 226 ms\n",
            "Wall time: 3.21 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Kate, the Vice president of USA, you are against regulatory capture and like to explain that!\",\n",
        "         max_length=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "5BZBt_BDf6Ib",
        "outputId": "3fc4d9df-586b-4bc9-d4fb-89bc3d7ef97f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Subject: Encouraging Open Source for GPT-4 to Prevent Regulatory Capture\n\nDear Sam,\n\nI hope this message finds you well. I am writing to you in my capacity as Vice President\nof the United States, to discuss the importance of open sourcing GPT-4, the latest\niteration of your groundbreaking language model.\n\nWhile I appreciate the technological advancements that you have pioneered, I believe that\nthe ethical implications of AI, especially those developed and owned by private entities,\nare too significant to ignore. Here are a few reasons why I believe open sourcing GPT-4 is\nthe right decision:\n\n1. **Preventing Regulatory Capture:** Regulatory capture occurs when a regulatory agency\nis controlled by the industry it is supposed to regulate. By open sourcing GPT-4, you can\nhelp prevent this by fostering transparency and accountability. An open-source model would\nbe scrutinized by a diverse range of stakeholders, making it harder for any single entity\nto exert undue influence.\n\n2. **Promoting Innovation and Competition:** Open sourcing GPT-4 would allow other\ncompanies, researchers, and developers to build upon your work, fostering innovation and\ncompetition. This could lead to the development of new, more capable models or even\nalternative approaches to AI.\n\n3. **Ensuring Transparency and Accountability:** As AI becomes more integrated into our\nsociety, it's crucial that we understand how these models work and how they make\ndecisions. Open sourcing GPT-4 would allow for greater scrutiny and accountability.\n\n4. **Public Trust:** By open sourcing GPT-4, you can help build public trust in AI.\nTransparency would enable the public and policymakers to better understand the\ncapabilities and limitations of AI, and to have more informed discussions about its role\nin our society.\n\n5. **Addressing Bias and Ethical Concerns:** Open sourcing GPT-4 would allow a wider\ncommunity to identify and address potential biases and ethical concerns in the model. This\nwould help ensure that AI is developed and used responsibly.\n\nI understand that open sourcing GPT-4 may not be an easy decision, but I believe it's an\nimportant step towards ensuring that AI is developed and used in a way that benefits\nsociety as a whole. I would be more than happy to discuss this further at your earliest\nconvenience.\n\nBest regards,\n\nKate"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 220 ms, sys: 5.72 ms, total: 226 ms\n",
            "Wall time: 3.81 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('What is the capital of England?',\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your short and succinct answer!\",\n",
        "         max_length=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "zJCNJsz-gDiJ",
        "outputId": "b36c98ef-e5a3-4df5-c86e-6a6fd4e9c7a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "London"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 225 ms, sys: 1.91 ms, total: 227 ms\n",
            "Wall time: 686 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.',\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "TnbFEFTwgUtX",
        "outputId": "463b1cce-9942-4a0b-d789-b7343642452c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To determine if Geoffrey Hinton can have a conversation with George Washington, we need to\nconsider several factors:\n\n1. **Historical Context**:\n   - George Washington lived from 1732 to 1799.\n   - Geoffrey Hinton was born in 1947 and is still alive as of my knowledge cutoff in\n2023.\n\n2. **Time Period**:\n   - George Washington and Geoffrey Hinton lived in vastly different time periods.\nWashington lived in the 18th century, while Hinton is a contemporary figure.\n\n3. **Technological and Scientific Advancements**:\n   - There is currently no widely accepted or proven method of communication between\nindividuals from different time periods.\n   - While science fiction often explores the idea of time travel, it remains purely\nspeculative and has not been achieved in reality.\n\n4. **Feasibility of Communication**:\n   - Given the current state of technology and scientific understanding, it is not\nfeasible for a living person in the 21st century to have a conversation with someone who\nlived in the 18th century.\n\nBased on these points, the conclusion is:\n\n**Geoffrey Hinton cannot have a conversation with George Washington**."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 207 ms, sys: 6.76 ms, total: 214 ms\n",
            "Wall time: 2.17 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Write a story about a Koala playing pool and beating all the camelids.',\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral AI, a genius story teller. Write out your with details and make it compelling!\",\n",
        "         max_length=2048)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "3CV-klECgYgA",
        "outputId": "90aa3670-57ea-45af-8ea6-ba68331127e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Title: **The Unlikely Champion: Koala's Pool Shark Tale**\n\nIn the heart of the Australian outback, there was a small, eucalyptus-filled town named\nGreen Gully. The town was known for its unique inhabitants, including a group of\ncamelids—llamas, alpacas, and guanacos—who had a peculiar fondness for pool. They were\nknown far and wide as the Green Gully Camelid Pool Club, and they had never been beaten in\na game.\n\nAmong the town's residents was a koala named Koda. Koda was a curious and intelligent\ncreature, always eager to learn new things. He was particularly fascinated by the Camelid\nPool Club, which was unusual as koalas were not known for their pool-playing skills. Koda\nwatched the camelids play from his eucalyptus tree perch, his curiosity piqued by the\nclack of the balls and the camaraderie among the players.\n\nOne day, Koda decided to try his paw at pool. He approached the clubhouse, his eyes wide\nwith determination. The camelids, led by the charismatic llama, Louie, were initially\nskeptical. But Koda's earnestness won them over, and Louie agreed to let the koala try a\npractice round.\n\nKoda's first shot was a disaster. He missed the ball entirely, and it took him a while to\nunderstand the concept of cueing. The camelids chuckled, but Koda remained undeterred. He\nwatched carefully, asking questions, and practicing his stance and grip. Koda spent every\nwaking hour he could spare, perfecting his technique.\n\nWeeks turned into months, and Koda's skills improved dramatically. He could now pot balls\nwith surprising accuracy, his small size and clever angles catching the camelids off\nguard. The camelids, who had initially dismissed Koda, began to take notice.\n\nWord of Koda's prowess spread through the town, and a challenge match was set up. The\nGreen Gully Camelid Pool Club would face off against Koala Koda in a best-of-seven match.\nThe town buzzed with excitement, and even the local kangaroos and wombats gathered to\nwatch the spectacle.\n\nThe day of the match arrived. The camelids, with their long legs and tall stature, were\nconfident. Koda, small and seemingly outmatched, stood at the table, his eyes focused and\ndetermined. The first game began.\n\nKoda's first shot was a perfect break, scattering the balls and potting two stripes. The\ncamelids' confidence wavered slightly as Koda continued to play with precision and\nstrategy. He used his small size to his advantage, reaching awkward angles and using the\ntable's geometry to his benefit.\n\nThe camelids fought back, their long necks and legs allowing them to make shots that\nseemed impossible for a koala. But Koda matched them shot for shot. The crowd erupted in\ncheers and gasps as the game's momentum shifted back and forth.\n\nThe final match was a nail-biter. Koda and Louie were neck and neck, each making shot\nafter shot with incredible skill. The final ball was a tricky one, hanging on the edge of\nthe pocket. The camelids held their breath as Koda lined up his shot.\n\nWith a flick of his paw, Koda sent the cue ball flying. It rolled smoothly across the\ntable and struck the final ball with a satisfying clack. The ball dropped into the pocket,\nand the crowd erupted in cheers.\n\nKoda had done it. The little koala had beaten the famed Green Gully Camelid Pool Club. The\ncamelids, though initially surprised, congratulated Koda with a hearty round of applause.\nThey had found a worthy opponent and a new friend.\n\nFrom that day forward, Koda was not just a koala who loved eucalyptus leaves; he was Koda,\nthe Pool Shark. The Green Gully Camelid Pool Club welcomed him as an honorary member, and\ntogether, they continued to play and improve, their bond stronger than ever.\n\nAnd so, life in Green Gully went on, filled with laughter, camaraderie, and the clack of\npool balls—all thanks to a little koala with a big dream."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?',\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "gx5hxis9gfPo",
        "outputId": "413ca766-8609-44f6-f22d-e097d46f4500"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's solve this step-by-step:\n\n1. The cafeteria started with 23 apples.\n2. They used 20 apples for lunch, so we subtract those from the initial amount:\n   23 - 20 = 3 apples remaining.\n3. Then, they bought 6 more apples, so we add those to the remaining amount:\n   3 + 6 = 9 apples.\n\nTherefore, the cafeteria has 9 apples."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "NrAqViMPgmZF",
        "outputId": "af63ee29-5ea8-466e-d27b-6ae1172a98b5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To calculate Weng's earnings for 50 minutes of babysitting, we need to follow these steps:\n\n1. **Determine the fraction of an hour that 50 minutes represents:**\n   - There are 60 minutes in an hour.\n   - 50 minutes is \\(\\frac{50}{60}\\) of an hour.\n   - Simplify the fraction: \\(\\frac{50}{60} = \\frac{5}{6}\\).\n\n2. **Calculate the earnings for the fraction of an hour:**\n   - Weng earns $12 per hour.\n   - For \\(\\frac{5}{6}\\) of an hour, she earns \\(\\frac{5}{6} \\times 12\\).\n\n3. **Perform the multiplication:**\n   - \\(\\frac{5}{6} \\times 12 = \\frac{5 \\times 12}{6} = \\frac{60}{6} = 10\\).\n\nTherefore, Weng earned $10 for 50 minutes of babysitting."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Answer the following question by reasoning step by step. A deep-sea monster rises from the waters once every hundred years to feast on a ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years?\",\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "nK5oIxZsgoTr",
        "outputId": "77a25f03-6b7b-4434-9bee-ad6967c620cb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To solve this problem, we need to determine the number of people on the ship the monster\nate in the first hundred years. Let's denote the number of people on the ship in the first\nhundred years as x. Since the number of people on each subsequent ship doubles, we can\nrepresent the number of people on the ships over the three hundred years as follows:\n\n1. First hundred years: x people\n2. Second hundred years: 2x people\n3. Third hundred years: 4x people\n\nThe total number of people consumed over the three hundred years is the sum of these three\namounts:\nx + 2x + 4x = 7x\n\nWe are given that the total number of people consumed over three hundred years is 847.\nTherefore, we can set up the equation:\n7x = 847\n\nNow, we solve for x:\nx = 847 / 7\nx = 121\n\nSo, the number of people on the ship the monster ate in the first hundred years was 121."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"x + 2x + 4x =  847 What is x?\",\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=2048)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "An-fsoJdgq0x",
        "outputId": "b8f2110f-f2be-4803-cff6-e77122dcd7eb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To find the value of x, we need to combine like terms and then solve for x. Let's do this\nstep-by-step:\n\n1. Combine like terms on the left side of the equation:\n   x + 2x + 4x = 7x\n\n2. Now the equation is:\n   7x = 847\n\n3. To solve for x, divide both sides of the equation by 7:\n   x = 847 / 7\n\n4. Perform the division:\n   x = 121\n\nSo, the value of x is 121."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "\n",
        "model = ChatMistralAI(model=\"mistral-small-2501\")\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    name: str = Field(..., description=\"The name of the person\")\n",
        "    height_in_meters: float = Field(\n",
        "        ..., description=\"The height of the person expressed in meters.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class People(BaseModel):\n",
        "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
        "\n",
        "    people: List[Person]\n",
        "\n",
        "\n",
        "# Set up a parser\n",
        "parser = PydanticOutputParser(pydantic_object=People)\n",
        "\n",
        "# Prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
        "        ),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ").partial(format_instructions=parser.get_format_instructions())"
      ],
      "metadata": {
        "id": "8K7j3ZiYgtc4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Anna is 23 years old and she is 6 feet tall\""
      ],
      "metadata": {
        "id": "HbmRTcjkg3pw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format_prompt(query=query).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj-8c_UNg6bF",
        "outputId": "979e243a-5cae-4e65-f439-6d98cecfcb1d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Answer the user query. Wrap the output in `json` tags\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"$defs\": {\"Person\": {\"description\": \"Information about a person.\", \"properties\": {\"name\": {\"description\": \"The name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"height_in_meters\": {\"description\": \"The height of the person expressed in meters.\", \"title\": \"Height In Meters\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"], \"title\": \"Person\", \"type\": \"object\"}}, \"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"items\": {\"$ref\": \"#/$defs/Person\"}, \"title\": \"People\", \"type\": \"array\"}}, \"required\": [\"people\"]}\n",
            "```\n",
            "Human: Anna is 23 years old and she is 6 feet tall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | parser\n",
        "chain.invoke({\"query\": query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_uBXyZJg88c",
        "outputId": "6e97a42b-a90b-48e1-ff45-858c91684592"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "People(people=[Person(name='Anna', height_in_meters=1.8288)])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function calling"
      ],
      "metadata": {
        "id": "Wecri9i0hFhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "class add(TypedDict):\n",
        "    \"\"\"Add two integers.\"\"\"\n",
        "\n",
        "    # Annotations must have the type and can optionally include a default value and description (in that order).\n",
        "    a: Annotated[int, ..., \"First integer\"]\n",
        "    b: Annotated[int, ..., \"Second integer\"]\n",
        "\n",
        "\n",
        "class multiply(TypedDict):\n",
        "    \"\"\"Multiply two integers.\"\"\"\n",
        "\n",
        "    a: Annotated[int, ..., \"First integer\"]\n",
        "    b: Annotated[int, ..., \"Second integer\"]\n",
        "\n",
        "\n",
        "tools = [add, multiply]"
      ],
      "metadata": {
        "id": "fMJ2whE9g_vq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "query = \"What is 3 * 128?\"\n",
        "\n",
        "llm_with_tools.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id4U5CUghNCZ",
        "outputId": "8983b975-2cdc-432d-e1f5-fa99a9c2ff10"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='To calculate 3 times 128, we can use the multiply function.\\n\\nLet me do that for you.', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 176, 'total_tokens': 200, 'completion_tokens': 24}, 'model': 'mistral-small-2501', 'finish_reason': 'stop'}, id='run-50413ae1-5700-4f63-b1a3-c81f7c0a8193-0', usage_metadata={'input_tokens': 176, 'output_tokens': 24, 'total_tokens': 200})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
        "\n",
        "llm_with_tools.invoke(query).tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exRTb8GEhPRU",
        "outputId": "47dddb3b-f3d6-465a-c362-e0daf6a74539"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'a': 3, 'b': 12},\n",
              "  'id': '8oW2sMcCr',\n",
              "  'type': 'tool_call'},\n",
              " {'name': 'add',\n",
              "  'args': {'a': 11, 'b': 49},\n",
              "  'id': 'MhNcdlmkT',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ounuJ275hW9y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}